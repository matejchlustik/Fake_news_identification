{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"full_TRAINING_DATASET.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>In these trying times, Jackie Mason is the Voi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69293</th>\n",
       "      <td>1</td>\n",
       "      <td>There are a larger number of shark attacks in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69294</th>\n",
       "      <td>1</td>\n",
       "      <td>Democrats have now become the party of the [At...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69295</th>\n",
       "      <td>1</td>\n",
       "      <td>Says an alternative to Social Security that op...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69296</th>\n",
       "      <td>0</td>\n",
       "      <td>On lifting the U.S. Cuban embargo and allowing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69297</th>\n",
       "      <td>0</td>\n",
       "      <td>The Department of Veterans Affairs has a manua...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69298 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                               text\n",
       "0          0  House Dem Aide: We Didn’t Even See Comey’s Let...\n",
       "1          1  Ever get the feeling your life circles the rou...\n",
       "2          0  Why the Truth Might Get You Fired October 29, ...\n",
       "3          0  Print \\nAn Iranian woman has been sentenced to...\n",
       "4          1  In these trying times, Jackie Mason is the Voi...\n",
       "...      ...                                                ...\n",
       "69293      1  There are a larger number of shark attacks in ...\n",
       "69294      1  Democrats have now become the party of the [At...\n",
       "69295      1  Says an alternative to Social Security that op...\n",
       "69296      0  On lifting the U.S. Cuban embargo and allowing...\n",
       "69297      0  The Department of Veterans Affairs has a manua...\n",
       "\n",
       "[69298 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matej\\SKOLA\\Diplomovka\\Code\\EmbeddingModels\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pandas as pd\n",
    "from time import time\n",
    "\n",
    "# Load tokenizer and model\n",
    "model_name = \"FacebookAI/roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r\"[‘’]\", \"'\", text)\n",
    "    return re.sub(r\"[“”`]\", '\"', text)\n",
    "\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return tokenizer(text, return_tensors=\"pt\", truncation=False)\n",
    "\n",
    "\n",
    "def chunk_and_pad_tokenized_input(inputs, max_length=512):\n",
    "\n",
    "    input_ids = inputs[\"input_ids\"][0]\n",
    "    attention_mask = inputs[\"attention_mask\"][0]\n",
    "    chunks = []\n",
    "\n",
    "    for i in range(0, len(input_ids), max_length):\n",
    "        chunk_input_ids = input_ids[i : i + max_length]\n",
    "        chunk_attention_mask = attention_mask[i : i + max_length]\n",
    "\n",
    "        if len(chunk_input_ids) < max_length:\n",
    "            padding_length = max_length - len(chunk_input_ids)\n",
    "            chunk_input_ids = torch.cat(\n",
    "                [\n",
    "                    chunk_input_ids,\n",
    "                    torch.tensor([tokenizer.pad_token_id] * padding_length),\n",
    "                ]\n",
    "            )\n",
    "            chunk_attention_mask = torch.cat(\n",
    "                [chunk_attention_mask, torch.tensor([0] * padding_length)]\n",
    "            )\n",
    "\n",
    "        chunks.append(\n",
    "            {\n",
    "                \"input_ids\": chunk_input_ids.unsqueeze(0),\n",
    "                \"attention_mask\": chunk_attention_mask.unsqueeze(0),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def get_embeddings_and_average(chunks, padding_token_id=0):\n",
    "    all_embeddings = []\n",
    "    for chunk in chunks:\n",
    "        chunk = {key: value.to(\"cuda\") for key, value in chunk.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**chunk)\n",
    "\n",
    "        token_embeddings = outputs.last_hidden_state\n",
    "\n",
    "        attention_mask = chunk[\"attention_mask\"]\n",
    "\n",
    "        padding_positions = (attention_mask == padding_token_id).nonzero(as_tuple=True)[\n",
    "            1\n",
    "        ]\n",
    "        if len(padding_positions) > 0:\n",
    "            first_padding_position = padding_positions[0].item()\n",
    "\n",
    "            token_embeddings = token_embeddings[:, :first_padding_position, :]\n",
    "        all_embeddings.append(token_embeddings)\n",
    "\n",
    "    all_embeddings = torch.cat(all_embeddings, dim=1)\n",
    "\n",
    "    final_embedding = all_embeddings.mean(dim=1)\n",
    "\n",
    "    return final_embedding\n",
    "\n",
    "\n",
    "def get_roberta_embedding(text, max_length=512):\n",
    "\n",
    "    text = preprocess_text(text)\n",
    "    inputs = tokenize_text(text)\n",
    "\n",
    "    chunks = chunk_and_pad_tokenized_input(inputs, max_length)\n",
    "\n",
    "    final_embedding = get_embeddings_and_average(chunks)\n",
    "\n",
    "    return final_embedding[0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ever get the feeling your life circles the roundabout rather than heads in a straight line toward the intended destination? [Hillary Clinton remains the big woman on campus in leafy, liberal Wellesley, Massachusetts. Everywhere else votes her most likely to don her inauguration dress for the remainder of her days the way Miss Havisham forever wore that wedding dress.  Speaking of Great Expectations, Hillary Rodham overflowed with them 48 years ago when she first addressed a Wellesley graduating class. The president of the college informed those gathered in 1969 that the students needed \"no debate so far as I could ascertain as to who their spokesman was to be\" (kind of the like the Democratic primaries in 2016 minus the   terms unknown then even at a Seven Sisters school). \"I am very glad that Miss Adams made it clear that what I am speaking for today is all of us —  the 400 of us,\" Miss Rodham told her classmates. After appointing herself Edger Bergen to the Charlie McCarthys and Mortimer Snerds in attendance, the    bespectacled in granny glasses (awarding her matronly wisdom —  or at least John Lennon wisdom) took issue with the previous speaker. Despite becoming the first   to win election to a seat in the U. S. Senate since Reconstruction, Edward Brooke came in for criticism for calling for \"empathy\" for the goals of protestors as he criticized tactics. Though Clinton in her senior thesis on Saul Alinsky lamented \"Black Power demagogues\" and \"elitist arrogance and repressive intolerance\" within the New Left, similar words coming out of a Republican necessitated a brief rebuttal. \"Trust,\" Rodham ironically observed in 1969, \"this is one word that when I asked the class at our rehearsal what it was they wanted me to say for them, everyone came up to me and said \\'Talk about trust, talk about the lack of trust both for us and the way we feel about others. Talk about the trust bust.\\' What can you say about it? What can you say about a feeling that permeates a generation and that perhaps is not even understood by those who are distrusted?\" The \"trust bust\" certainly busted Clinton\\'s 2016 plans. She certainly did not even understand that people distrusted her. After Whitewater, Travelgate, the vast   conspiracy, Benghazi, and the missing emails, Clinton found herself the distrusted voice on Friday. There was a load of compromising on the road to the broadening of her political horizons. And distrust from the American people —  Trump edged her 48 percent to 38 percent on the question immediately prior to November\\'s election —  stood as a major reason for the closing of those horizons. Clinton described her vanquisher and his supporters as embracing a \"lie,\" a \"con,\" \"alternative facts,\" and \"a   assault on truth and reason. \" She failed to explain why the American people chose his lies over her truth. \"As the history majors among you here today know all too well, when people in power invent their own facts and attack those who question them, it can mark the beginning of the end of a free society,\" she offered. \"That is not hyperbole. \" Like so many people to emerge from the 1960s, Hillary Clinton embarked upon a long, strange trip. From high school Goldwater Girl and Wellesley College Republican president to Democratic politician, Clinton drank in the times and the place that gave her a degree. More significantly, she went from idealist to cynic, as a comparison of her two Wellesley commencement addresses show. Way back when, she lamented that \"for too long our leaders have viewed politics as the art of the possible, and the challenge now is to practice politics as the art of making what appears to be impossible possible. \" Now, as the big woman on campus but the odd woman out of the White House, she wonders how her current station is even possible. \"Why aren\\'t I 50 points ahead?\" she asked in September. In May she asks why she isn\\'t president. The woman famously dubbed a \"congenital liar\" by Bill Safire concludes that lies did her in —  theirs, mind you, not hers. Getting stood up on Election Day, like finding yourself the jilted bride on your wedding day, inspires dangerous delusions.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = df.iloc[1][\"text\"]\n",
    "text = preprocess_text(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0, 45134, 19233,  6315,  2788,  1246,     4,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = tokenizer(\n",
    "    \"Random tokenizer text example.\", return_tensors=\"pt\", truncation=False\n",
    ")\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', 'Random', 'Ġtoken', 'izer', 'Ġtext', 'Ġexample', '.', '</s>']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(example[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (887 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0, 27172,   120,     5,  2157,   110,   301, 13820,     5,  1062,\n",
       "          9006,  1195,    87,  3885,    11,    10,  1359,   516,  1706,     5,\n",
       "          3833,  6381,   116,   646, 38655,  2235,  1189,     5,   380,   693,\n",
       "            15,  2894,    11, 14610,   219,     6,  6176,  2647, 36664,     6,\n",
       "          5517,     4, 37691,  1493,  2834,    69,   144,   533,     7,   218,\n",
       "            69,  9714,  3588,    13,     5, 11059,     9,    69,   360,     5,\n",
       "           169,  4523, 22989,  1173,   424,  6000,  5328,    14,  3312,  3588,\n",
       "             4,  1437,  3580,     9,  2860, 12809,  1635,     6,  5141, 46581,\n",
       "         41031,  9725,    19,   106,  2929,   107,   536,    77,    79,    78,\n",
       "          4873,    10,  2647, 36664, 15128,  1380,     4,    20,   394,     9,\n",
       "             5,  1564,  3978,   167,  4366,    11, 15077,    14,     5,   521,\n",
       "           956,    22,  2362,  2625,    98,   444,    25,    38,   115, 29477,\n",
       "            25,     7,    54,    49,  1565,    21,     7,    28,   113,    36,\n",
       "         11584,     9,     5,   101,     5,  1557, 19050,    11,   336, 10877,\n",
       "             5,  1437,  1437,  1110,  4727,   172,   190,    23,    10,  7732,\n",
       "         22316,   334,   322,    22,   100,   524,   182,  7785,    14,  4523,\n",
       "          5710,   156,    24,   699,    14,    99,    38,   524,  2686,    13,\n",
       "           452,    16,    70,     9,   201,    93,  1437,     5,  3675,     9,\n",
       "           201,    60,  4523, 46581,   174,    69, 18295,     4,   572, 25877,\n",
       "          2864,  2344,  2403,  4323,  4138,     7,     5,  6127,  3409, 16880,\n",
       "          2459,     8, 17457, 15677,   208,  1396, 11622,    11,  6856,     6,\n",
       "             5,  1437,  1437,  1437,  9988, 13771,  1043,  1329,    11,  4435,\n",
       "         13749, 11121,    36,  1584, 16014,    69,  7821,  2839,   352, 12320,\n",
       "            93,  1437,    50,    23,   513,   610, 23563, 12320,    43,   362,\n",
       "           696,    19,     5,   986,  5385,     4,  2285,  1959,     5,    78,\n",
       "          1437,  1437,     7,   339,   729,     7,    10,  2418,    11,     5,\n",
       "           121,     4,   208,     4,  1112,   187, 35411,     6,  7393, 17806,\n",
       "           376,    11,    13,  3633,    13,  1765,    13,    22,   991, 39214,\n",
       "           113,    13,     5,  1175,     9, 27992,    25,    37,  5888,  8893,\n",
       "             4,  3791,  2235,    11,    69,   949, 24149,    15, 23144,   726,\n",
       "         16444, 20010,    22, 11368,  3029,  4410, 33395,  3663,   113,     8,\n",
       "            22,   523,   405,   661, 32818,     8, 37868, 27488,   113,   624,\n",
       "             5,   188, 10039,     6,  1122,  1617,   567,    66,     9,    10,\n",
       "          1172, 25071, 15114,    10,  4315, 37862,   337,     4,    22, 18823,\n",
       "            60, 46581, 30829,  6373,    11, 15077,     6,    22,  9226,    16,\n",
       "            65,  2136,    14,    77,    38,   553,     5,  1380,    23,    84,\n",
       "         24478,    99,    24,    21,    51,   770,   162,     7,   224,    13,\n",
       "           106,     6,   961,   376,    62,     7,   162,     8,    26,   128,\n",
       "         27743,    59,  2416,     6,  1067,    59,     5,  1762,     9,  2416,\n",
       "           258,    13,   201,     8,     5,   169,    52,   619,    59,   643,\n",
       "             4, 10391,    59,     5,  2416, 11044,   955,   653,    64,    47,\n",
       "           224,    59,    24,   116,   653,    64,    47,   224,    59,    10,\n",
       "          2157,    14, 31582,  1626,    10,  2706,     8,    14,  2532,    16,\n",
       "            45,   190,  6238,    30,   167,    54,    32,  7018,   338, 15876,\n",
       "          1917,    20,    22, 23170, 11044,   113,  1819, 26803,  2235,    18,\n",
       "           336,   708,     4,   264,  1819,   222,    45,   190,  1346,    14,\n",
       "            82,  7018,   338, 15876,    69,     4,   572,  7559, 24159,     6,\n",
       "          8696,  7357,     6,     5,  4714,  1437,  1437,  6556,     6, 30473,\n",
       "             6,     8,     5,  1716,  5575,     6,  2235,   303,  2864,     5,\n",
       "          7018,   338, 15876,  2236,    15,   273,     4,   345,    21,    10,\n",
       "          7511,     9, 25985,    15,     5,   921,     7,     5,  4007,  4226,\n",
       "             9,    69,   559, 17211, 36645,     4,   178, 27948,    31,     5,\n",
       "           470,    82,    93,  1437,   140,  9897,    69,  2929,   135,     7,\n",
       "          2843,   135,    15,     5,   864,  1320,  2052,     7,   759,    18,\n",
       "           729,    93,  1437,  3359,    25,    10,   538,  1219,    13,     5,\n",
       "          3172,     9,   167, 17211, 36645,     4,  2235,  1602,    69,  3538,\n",
       "          2253, 13761,     8,    39,  2732,    25, 16105,    10,    22, 11527,\n",
       "            60,    10,    22,  3865,    60,    22, 34268,  3693,  4905,    60,\n",
       "             8,    22,   102,  1437,  1437,  2080,    15,  3157,     8,  1219,\n",
       "             4,    22,   264,  1447,     7,  3922,   596,     5,   470,    82,\n",
       "          4689,    39,  5738,    81,    69,  3157,     4,    22,  1620,     5,\n",
       "           750, 11466,   566,    47,   259,   452,   216,    70,   350,   157,\n",
       "             6,    77,    82,    11,   476, 11022,    49,   308,  4905,     8,\n",
       "           908,   167,    54,   864,   106,     6,    24,    64,  2458,     5,\n",
       "          1786,     9,     5,   253,     9,    10,   481,  2313,    60,    79,\n",
       "          1661,     4,    22,  1711,    16,    45,  8944, 38373,     4,    22,\n",
       "          2011,    98,   171,    82,     7,  9845,    31,     5,  7571,    29,\n",
       "             6,  5141,  2235, 19179,  2115,    10,   251,     6,  7782,  1805,\n",
       "             4,  1740,   239,   334,  2610,  5412,  9103,     8,  2647, 36664,\n",
       "          1821,  1172,   394,     7,  1557,  8676,     6,  2235, 24313,    11,\n",
       "             5,   498,     8,     5,   317,    14,   851,    69,    10,  3093,\n",
       "             4,   901,  3625,     6,    79,   439,    31,  5631,   661,     7,\n",
       "         40240,   636,     6,    25,    10,  6676,     9,    69,    80,  2647,\n",
       "         36664, 23666,  8480,   311,     4,  4846,   124,    77,     6,    79,\n",
       "         20010,    14,    22,  1990,   350,   251,    84,   917,    33,  5915,\n",
       "          2302,    25,     5,  1808,     9,     5,   678,     6,     8,     5,\n",
       "          1539,   122,    16,     7,  1524,  2302,    25,     5,  1808,     9,\n",
       "           442,    99,  2092,     7,    28,  4703,   678,     4,    22,   978,\n",
       "             6,    25,     5,   380,   693,    15,  2894,    53,     5,  8372,\n",
       "           693,    66,     9,     5,   735,   446,     6,    79, 17694,   141,\n",
       "            69,   595,  1992,    16,   190,   678,     4,    22,  7608,  2025,\n",
       "            75,    38,   654,   332,   789,  1917,    79,   553,    11,   772,\n",
       "             4,    96,   392,    79,  6990,   596,    79,   965,    75,   394,\n",
       "             4,    20,   693, 13455,  9260,    10,    22, 37519,   225,  8632,\n",
       "         28587,   113,    30,  1585, 11881,  1885, 14372,    14,  5738,   222,\n",
       "            69,    11,    93,  1437, 20343,     6,  1508,    47,     6,    45,\n",
       "         23367,     4, 11812,  3359,    62,    15,  7713,  1053,     6,   101,\n",
       "          2609,  2512,     5,  1236,   718,  5357, 15429,    15,   110,  3312,\n",
       "           183,     6, 25362,  2702, 43267,     4,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = tokenize_text(text)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input_ids': tensor([[    0, 27172,   120,     5,  2157,   110,   301, 13820,     5,  1062,\n",
       "            9006,  1195,    87,  3885,    11,    10,  1359,   516,  1706,     5,\n",
       "            3833,  6381,   116,   646, 38655,  2235,  1189,     5,   380,   693,\n",
       "              15,  2894,    11, 14610,   219,     6,  6176,  2647, 36664,     6,\n",
       "            5517,     4, 37691,  1493,  2834,    69,   144,   533,     7,   218,\n",
       "              69,  9714,  3588,    13,     5, 11059,     9,    69,   360,     5,\n",
       "             169,  4523, 22989,  1173,   424,  6000,  5328,    14,  3312,  3588,\n",
       "               4,  1437,  3580,     9,  2860, 12809,  1635,     6,  5141, 46581,\n",
       "           41031,  9725,    19,   106,  2929,   107,   536,    77,    79,    78,\n",
       "            4873,    10,  2647, 36664, 15128,  1380,     4,    20,   394,     9,\n",
       "               5,  1564,  3978,   167,  4366,    11, 15077,    14,     5,   521,\n",
       "             956,    22,  2362,  2625,    98,   444,    25,    38,   115, 29477,\n",
       "              25,     7,    54,    49,  1565,    21,     7,    28,   113,    36,\n",
       "           11584,     9,     5,   101,     5,  1557, 19050,    11,   336, 10877,\n",
       "               5,  1437,  1437,  1110,  4727,   172,   190,    23,    10,  7732,\n",
       "           22316,   334,   322,    22,   100,   524,   182,  7785,    14,  4523,\n",
       "            5710,   156,    24,   699,    14,    99,    38,   524,  2686,    13,\n",
       "             452,    16,    70,     9,   201,    93,  1437,     5,  3675,     9,\n",
       "             201,    60,  4523, 46581,   174,    69, 18295,     4,   572, 25877,\n",
       "            2864,  2344,  2403,  4323,  4138,     7,     5,  6127,  3409, 16880,\n",
       "            2459,     8, 17457, 15677,   208,  1396, 11622,    11,  6856,     6,\n",
       "               5,  1437,  1437,  1437,  9988, 13771,  1043,  1329,    11,  4435,\n",
       "           13749, 11121,    36,  1584, 16014,    69,  7821,  2839,   352, 12320,\n",
       "              93,  1437,    50,    23,   513,   610, 23563, 12320,    43,   362,\n",
       "             696,    19,     5,   986,  5385,     4,  2285,  1959,     5,    78,\n",
       "            1437,  1437,     7,   339,   729,     7,    10,  2418,    11,     5,\n",
       "             121,     4,   208,     4,  1112,   187, 35411,     6,  7393, 17806,\n",
       "             376,    11,    13,  3633,    13,  1765,    13,    22,   991, 39214,\n",
       "             113,    13,     5,  1175,     9, 27992,    25,    37,  5888,  8893,\n",
       "               4,  3791,  2235,    11,    69,   949, 24149,    15, 23144,   726,\n",
       "           16444, 20010,    22, 11368,  3029,  4410, 33395,  3663,   113,     8,\n",
       "              22,   523,   405,   661, 32818,     8, 37868, 27488,   113,   624,\n",
       "               5,   188, 10039,     6,  1122,  1617,   567,    66,     9,    10,\n",
       "            1172, 25071, 15114,    10,  4315, 37862,   337,     4,    22, 18823,\n",
       "              60, 46581, 30829,  6373,    11, 15077,     6,    22,  9226,    16,\n",
       "              65,  2136,    14,    77,    38,   553,     5,  1380,    23,    84,\n",
       "           24478,    99,    24,    21,    51,   770,   162,     7,   224,    13,\n",
       "             106,     6,   961,   376,    62,     7,   162,     8,    26,   128,\n",
       "           27743,    59,  2416,     6,  1067,    59,     5,  1762,     9,  2416,\n",
       "             258,    13,   201,     8,     5,   169,    52,   619,    59,   643,\n",
       "               4, 10391,    59,     5,  2416, 11044,   955,   653,    64,    47,\n",
       "             224,    59,    24,   116,   653,    64,    47,   224,    59,    10,\n",
       "            2157,    14, 31582,  1626,    10,  2706,     8,    14,  2532,    16,\n",
       "              45,   190,  6238,    30,   167,    54,    32,  7018,   338, 15876,\n",
       "            1917,    20,    22, 23170, 11044,   113,  1819, 26803,  2235,    18,\n",
       "             336,   708,     4,   264,  1819,   222,    45,   190,  1346,    14,\n",
       "              82,  7018,   338, 15876,    69,     4,   572,  7559, 24159,     6,\n",
       "            8696,  7357,     6,     5,  4714,  1437,  1437,  6556,     6, 30473,\n",
       "               6,     8,     5,  1716,  5575,     6,  2235,   303,  2864,     5,\n",
       "            7018,   338, 15876,  2236,    15,   273,     4,   345,    21,    10,\n",
       "            7511,     9, 25985,    15,     5,   921,     7,     5,  4007,  4226,\n",
       "               9,    69]]),\n",
       "  'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1]])},\n",
       " {'input_ids': tensor([[  559, 17211, 36645,     4,   178, 27948,    31,     5,   470,    82,\n",
       "              93,  1437,   140,  9897,    69,  2929,   135,     7,  2843,   135,\n",
       "              15,     5,   864,  1320,  2052,     7,   759,    18,   729,    93,\n",
       "            1437,  3359,    25,    10,   538,  1219,    13,     5,  3172,     9,\n",
       "             167, 17211, 36645,     4,  2235,  1602,    69,  3538,  2253, 13761,\n",
       "               8,    39,  2732,    25, 16105,    10,    22, 11527,    60,    10,\n",
       "              22,  3865,    60,    22, 34268,  3693,  4905,    60,     8,    22,\n",
       "             102,  1437,  1437,  2080,    15,  3157,     8,  1219,     4,    22,\n",
       "             264,  1447,     7,  3922,   596,     5,   470,    82,  4689,    39,\n",
       "            5738,    81,    69,  3157,     4,    22,  1620,     5,   750, 11466,\n",
       "             566,    47,   259,   452,   216,    70,   350,   157,     6,    77,\n",
       "              82,    11,   476, 11022,    49,   308,  4905,     8,   908,   167,\n",
       "              54,   864,   106,     6,    24,    64,  2458,     5,  1786,     9,\n",
       "               5,   253,     9,    10,   481,  2313,    60,    79,  1661,     4,\n",
       "              22,  1711,    16,    45,  8944, 38373,     4,    22,  2011,    98,\n",
       "             171,    82,     7,  9845,    31,     5,  7571,    29,     6,  5141,\n",
       "            2235, 19179,  2115,    10,   251,     6,  7782,  1805,     4,  1740,\n",
       "             239,   334,  2610,  5412,  9103,     8,  2647, 36664,  1821,  1172,\n",
       "             394,     7,  1557,  8676,     6,  2235, 24313,    11,     5,   498,\n",
       "               8,     5,   317,    14,   851,    69,    10,  3093,     4,   901,\n",
       "            3625,     6,    79,   439,    31,  5631,   661,     7, 40240,   636,\n",
       "               6,    25,    10,  6676,     9,    69,    80,  2647, 36664, 23666,\n",
       "            8480,   311,     4,  4846,   124,    77,     6,    79, 20010,    14,\n",
       "              22,  1990,   350,   251,    84,   917,    33,  5915,  2302,    25,\n",
       "               5,  1808,     9,     5,   678,     6,     8,     5,  1539,   122,\n",
       "              16,     7,  1524,  2302,    25,     5,  1808,     9,   442,    99,\n",
       "            2092,     7,    28,  4703,   678,     4,    22,   978,     6,    25,\n",
       "               5,   380,   693,    15,  2894,    53,     5,  8372,   693,    66,\n",
       "               9,     5,   735,   446,     6,    79, 17694,   141,    69,   595,\n",
       "            1992,    16,   190,   678,     4,    22,  7608,  2025,    75,    38,\n",
       "             654,   332,   789,  1917,    79,   553,    11,   772,     4,    96,\n",
       "             392,    79,  6990,   596,    79,   965,    75,   394,     4,    20,\n",
       "             693, 13455,  9260,    10,    22, 37519,   225,  8632, 28587,   113,\n",
       "              30,  1585, 11881,  1885, 14372,    14,  5738,   222,    69,    11,\n",
       "              93,  1437, 20343,     6,  1508,    47,     6,    45, 23367,     4,\n",
       "           11812,  3359,    62,    15,  7713,  1053,     6,   101,  2609,  2512,\n",
       "               5,  1236,   718,  5357, 15429,    15,   110,  3312,   183,     6,\n",
       "           25362,  2702, 43267,     4,     2,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1]]),\n",
       "  'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0]])}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = chunk_and_pad_tokenized_input(out)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks[-1][\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks[-1][\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = get_embeddings_and_average(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "887"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"article_embedding\"] = df[\"text\"].apply(lambda x: get_roberta_embedding(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>article_embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>[0.0028758862, 0.025872586, 0.03593416, -0.099...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>[0.032413572, 0.035666395, 0.057574242, -0.100...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>[0.037595626, 0.05027227, 0.060264364, -0.0506...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>[0.028826052, 0.057950247, -0.00472429, -0.082...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>In these trying times, Jackie Mason is the Voi...</td>\n",
       "      <td>[-0.02413057, 0.03461668, 0.060766198, -0.1634...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69293</th>\n",
       "      <td>1</td>\n",
       "      <td>There are a larger number of shark attacks in ...</td>\n",
       "      <td>[0.038691726, 0.14385675, 0.050323993, -0.0680...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69294</th>\n",
       "      <td>1</td>\n",
       "      <td>Democrats have now become the party of the [At...</td>\n",
       "      <td>[-0.07634911, 0.035728134, 0.058021225, -0.027...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69295</th>\n",
       "      <td>1</td>\n",
       "      <td>Says an alternative to Social Security that op...</td>\n",
       "      <td>[0.04432549, 0.11710828, 0.010645155, -0.05531...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69296</th>\n",
       "      <td>0</td>\n",
       "      <td>On lifting the U.S. Cuban embargo and allowing...</td>\n",
       "      <td>[0.0734633, 0.14546232, 0.10696487, -0.0214499...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69297</th>\n",
       "      <td>0</td>\n",
       "      <td>The Department of Veterans Affairs has a manua...</td>\n",
       "      <td>[6.1149534e-05, 0.12721843, 0.056653425, -0.06...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69298 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                               text  \\\n",
       "0          0  House Dem Aide: We Didn’t Even See Comey’s Let...   \n",
       "1          1  Ever get the feeling your life circles the rou...   \n",
       "2          0  Why the Truth Might Get You Fired October 29, ...   \n",
       "3          0  Print \\nAn Iranian woman has been sentenced to...   \n",
       "4          1  In these trying times, Jackie Mason is the Voi...   \n",
       "...      ...                                                ...   \n",
       "69293      1  There are a larger number of shark attacks in ...   \n",
       "69294      1  Democrats have now become the party of the [At...   \n",
       "69295      1  Says an alternative to Social Security that op...   \n",
       "69296      0  On lifting the U.S. Cuban embargo and allowing...   \n",
       "69297      0  The Department of Veterans Affairs has a manua...   \n",
       "\n",
       "                                       article_embedding  \n",
       "0      [0.0028758862, 0.025872586, 0.03593416, -0.099...  \n",
       "1      [0.032413572, 0.035666395, 0.057574242, -0.100...  \n",
       "2      [0.037595626, 0.05027227, 0.060264364, -0.0506...  \n",
       "3      [0.028826052, 0.057950247, -0.00472429, -0.082...  \n",
       "4      [-0.02413057, 0.03461668, 0.060766198, -0.1634...  \n",
       "...                                                  ...  \n",
       "69293  [0.038691726, 0.14385675, 0.050323993, -0.0680...  \n",
       "69294  [-0.07634911, 0.035728134, 0.058021225, -0.027...  \n",
       "69295  [0.04432549, 0.11710828, 0.010645155, -0.05531...  \n",
       "69296  [0.0734633, 0.14546232, 0.10696487, -0.0214499...  \n",
       "69297  [6.1149534e-05, 0.12721843, 0.056653425, -0.06...  \n",
       "\n",
       "[69298 rows x 3 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>article_embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>[0.0028758862, 0.025872586, 0.03593416, -0.099...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>[0.032413572, 0.035666395, 0.057574242, -0.100...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>[0.037595626, 0.05027227, 0.060264364, -0.0506...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>[0.028826052, 0.057950247, -0.00472429, -0.082...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>In these trying times, Jackie Mason is the Voi...</td>\n",
       "      <td>[-0.02413057, 0.03461668, 0.060766198, -0.1634...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69293</th>\n",
       "      <td>1</td>\n",
       "      <td>There are a larger number of shark attacks in ...</td>\n",
       "      <td>[0.038691726, 0.14385675, 0.050323993, -0.0680...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69294</th>\n",
       "      <td>1</td>\n",
       "      <td>Democrats have now become the party of the [At...</td>\n",
       "      <td>[-0.07634911, 0.035728134, 0.058021225, -0.027...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69295</th>\n",
       "      <td>1</td>\n",
       "      <td>Says an alternative to Social Security that op...</td>\n",
       "      <td>[0.04432549, 0.11710828, 0.010645155, -0.05531...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69296</th>\n",
       "      <td>0</td>\n",
       "      <td>On lifting the U.S. Cuban embargo and allowing...</td>\n",
       "      <td>[0.0734633, 0.14546232, 0.10696487, -0.0214499...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69297</th>\n",
       "      <td>0</td>\n",
       "      <td>The Department of Veterans Affairs has a manua...</td>\n",
       "      <td>[6.1149534e-05, 0.12721843, 0.056653425, -0.06...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69298 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                               text  \\\n",
       "0          0  House Dem Aide: We Didn’t Even See Comey’s Let...   \n",
       "1          1  Ever get the feeling your life circles the rou...   \n",
       "2          0  Why the Truth Might Get You Fired October 29, ...   \n",
       "3          0  Print \\nAn Iranian woman has been sentenced to...   \n",
       "4          1  In these trying times, Jackie Mason is the Voi...   \n",
       "...      ...                                                ...   \n",
       "69293      1  There are a larger number of shark attacks in ...   \n",
       "69294      1  Democrats have now become the party of the [At...   \n",
       "69295      1  Says an alternative to Social Security that op...   \n",
       "69296      0  On lifting the U.S. Cuban embargo and allowing...   \n",
       "69297      0  The Department of Veterans Affairs has a manua...   \n",
       "\n",
       "                                       article_embedding  \n",
       "0      [0.0028758862, 0.025872586, 0.03593416, -0.099...  \n",
       "1      [0.032413572, 0.035666395, 0.057574242, -0.100...  \n",
       "2      [0.037595626, 0.05027227, 0.060264364, -0.0506...  \n",
       "3      [0.028826052, 0.057950247, -0.00472429, -0.082...  \n",
       "4      [-0.02413057, 0.03461668, 0.060766198, -0.1634...  \n",
       "...                                                  ...  \n",
       "69293  [0.038691726, 0.14385675, 0.050323993, -0.0680...  \n",
       "69294  [-0.07634911, 0.035728134, 0.058021225, -0.027...  \n",
       "69295  [0.04432549, 0.11710828, 0.010645155, -0.05531...  \n",
       "69296  [0.0734633, 0.14546232, 0.10696487, -0.0214499...  \n",
       "69297  [6.1149534e-05, 0.12721843, 0.056653425, -0.06...  \n",
       "\n",
       "[69298 rows x 3 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "df[\"article_embedding\"] = df[\"article_embedding\"].apply(\n",
    "    lambda x: x.tolist() if isinstance(x, np.ndarray) else x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\n",
    "    \"embedded_datasets/RoBERTa/RoBERTa_Embedded_Dataset.csv\",\n",
    "    encoding=\"utf-8\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalization dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"full_GENERALIZATION_DATASET.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5985</th>\n",
       "      <td>1</td>\n",
       "      <td>The State Department told the Republican Natio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5986</th>\n",
       "      <td>0</td>\n",
       "      <td>The ‘P’ in PBS Should Stand for ‘Plutocratic’ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5987</th>\n",
       "      <td>0</td>\n",
       "      <td>Anti-Trump Protesters Are Tools of the Oligar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5988</th>\n",
       "      <td>1</td>\n",
       "      <td>ADDIS ABABA, Ethiopia —President Obama convene...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5989</th>\n",
       "      <td>1</td>\n",
       "      <td>Jeb Bush Is Suddenly Attacking Trump. Here's W...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5990 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text\n",
       "0         0  Daniel Greenfield, a Shillman Journalism Fello...\n",
       "1         0  Google Pinterest Digg Linkedin Reddit Stumbleu...\n",
       "2         1  U.S. Secretary of State John F. Kerry said Mon...\n",
       "3         0  — Kaydee King (@KaydeeKing) November 9, 2016 T...\n",
       "4         1  It's primary day in New York and front-runners...\n",
       "...     ...                                                ...\n",
       "5985      1  The State Department told the Republican Natio...\n",
       "5986      0  The ‘P’ in PBS Should Stand for ‘Plutocratic’ ...\n",
       "5987      0   Anti-Trump Protesters Are Tools of the Oligar...\n",
       "5988      1  ADDIS ABABA, Ethiopia —President Obama convene...\n",
       "5989      1  Jeb Bush Is Suddenly Attacking Trump. Here's W...\n",
       "\n",
       "[5990 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1549 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "df[\"article_embedding\"] = df[\"text\"].apply(lambda x: get_roberta_embedding(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>article_embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>[-0.002649596, 0.061043363, 0.052470032, -0.09...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>[0.0022733945, 0.03975339, 0.039920527, -0.113...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>[0.054484554, 0.11057529, 0.06774962, -0.06846...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>[-0.027517661, 0.05548241, 0.016553458, -0.064...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>[0.004216633, 0.09147257, 0.037530564, -0.0814...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5985</th>\n",
       "      <td>1</td>\n",
       "      <td>The State Department told the Republican Natio...</td>\n",
       "      <td>[0.0014618421, 0.11570988, 0.020603774, -0.064...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5986</th>\n",
       "      <td>0</td>\n",
       "      <td>The ‘P’ in PBS Should Stand for ‘Plutocratic’ ...</td>\n",
       "      <td>[0.014944622, 0.077357404, 0.10143796, -0.0913...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5987</th>\n",
       "      <td>0</td>\n",
       "      <td>Anti-Trump Protesters Are Tools of the Oligar...</td>\n",
       "      <td>[-0.007107026, 0.07709258, 0.08062994, -0.1241...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5988</th>\n",
       "      <td>1</td>\n",
       "      <td>ADDIS ABABA, Ethiopia —President Obama convene...</td>\n",
       "      <td>[0.03555587, 0.088559456, 0.043276478, -0.0736...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5989</th>\n",
       "      <td>1</td>\n",
       "      <td>Jeb Bush Is Suddenly Attacking Trump. Here's W...</td>\n",
       "      <td>[0.025442319, 0.057444483, 0.018110303, -0.093...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5990 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text  \\\n",
       "0         0  Daniel Greenfield, a Shillman Journalism Fello...   \n",
       "1         0  Google Pinterest Digg Linkedin Reddit Stumbleu...   \n",
       "2         1  U.S. Secretary of State John F. Kerry said Mon...   \n",
       "3         0  — Kaydee King (@KaydeeKing) November 9, 2016 T...   \n",
       "4         1  It's primary day in New York and front-runners...   \n",
       "...     ...                                                ...   \n",
       "5985      1  The State Department told the Republican Natio...   \n",
       "5986      0  The ‘P’ in PBS Should Stand for ‘Plutocratic’ ...   \n",
       "5987      0   Anti-Trump Protesters Are Tools of the Oligar...   \n",
       "5988      1  ADDIS ABABA, Ethiopia —President Obama convene...   \n",
       "5989      1  Jeb Bush Is Suddenly Attacking Trump. Here's W...   \n",
       "\n",
       "                                      article_embedding  \n",
       "0     [-0.002649596, 0.061043363, 0.052470032, -0.09...  \n",
       "1     [0.0022733945, 0.03975339, 0.039920527, -0.113...  \n",
       "2     [0.054484554, 0.11057529, 0.06774962, -0.06846...  \n",
       "3     [-0.027517661, 0.05548241, 0.016553458, -0.064...  \n",
       "4     [0.004216633, 0.09147257, 0.037530564, -0.0814...  \n",
       "...                                                 ...  \n",
       "5985  [0.0014618421, 0.11570988, 0.020603774, -0.064...  \n",
       "5986  [0.014944622, 0.077357404, 0.10143796, -0.0913...  \n",
       "5987  [-0.007107026, 0.07709258, 0.08062994, -0.1241...  \n",
       "5988  [0.03555587, 0.088559456, 0.043276478, -0.0736...  \n",
       "5989  [0.025442319, 0.057444483, 0.018110303, -0.093...  \n",
       "\n",
       "[5990 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "df[\"article_embedding\"] = df[\"article_embedding\"].apply(\n",
    "    lambda x: x.tolist() if isinstance(x, np.ndarray) else x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\n",
    "    \"embedded_datasets/RoBERTa/RoBERTa_Embedded_Generalization_Dataset.csv\",\n",
    "    encoding=\"utf-8\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
